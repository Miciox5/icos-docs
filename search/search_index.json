{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"get-started/installation/","title":"Installation","text":"<p>This document describes how to install RAG Application.</p> <p>We recommend you create a virtual environment for dependency isolation. See the Conda documentation or the Python documentation for details.</p>"},{"location":"get-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python and pip installed.</li> <li>Conda for virtual environment management.  </li> <li>Docker for simplify software application deployment processes.</li> </ul>"},{"location":"get-started/installation/#environment-setup","title":"Environment Setup","text":"<p>Clone the repo using git:</p> <pre><code>git clone https://github.com/Miciox5/ICOS.git\n</code></pre> <p>Create and activate a new virtual environment:</p> <pre><code>conda create -n ICOS python=3.10.0\nconda activate ICOS\n</code></pre> <p>Install the dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Warning</p> <p>If you encounter an error while building a wheel during the <code>pip install</code> process, you may need to install a C++ compiler on your computer.</p>"},{"location":"get-started/quickstart/","title":"Quickstart","text":"<p>This quickstart shows how to run RAG application. It allows you to familiarize yourself with the workflow and gain a basic understanding of the lifecycle. Specifically, you will perform the following tasks in this tutorial:</p> <ul> <li>Serve application with Docker</li> <li>Manage your local data</li> <li>Ask the application about your data</li> <li>Trace your application with LangSmith</li> </ul>"},{"location":"get-started/quickstart/#run-services-with-docker","title":"Run services with Docker","text":"<p>Docker simplifies application deployment by packaging software and its dependencies into containers, ensuring consistent operation across different environments. It enables developers to build, ship, and run applications seamlessly across various platforms.</p>"},{"location":"get-started/quickstart/#available-services","title":"Available services","text":"icos-mongodbmongo-expresslangfuse-serverredis-stack-chat-historiesopenllm <p><pre><code>cd services/\ndocker compose up icos-mongodb  -d\n</code></pre> This service provides a MongoDB database environment. Data stored within this <code>MongoDB</code> instance is mapped to a local data directory, likely for persistence. It serves as a backend data store for applications that require a NoSQL database solution.</p> <p><pre><code>cd services/\ndocker compose up mongo-express  -d\n</code></pre> This service is a web-based MongoDB administration interface. It is connected to the icos-mongodb service, likely to provide a graphical user interface (GUI) for managing MongoDB databases, collections, and documents. It facilitates easier interaction and management of MongoDB data without directly using command-line tools.</p> <p><pre><code>cd services/\ndocker compose up langfuse-server  -d\n</code></pre> LangFuse service is use for traces, evals, prompt management and metrics to debug and improve your LLM RAG application.</p> <p><pre><code>cd services/\ndocker compose up redis-stack-chat-histories  -d\n</code></pre> This service provides a Redis database environment for storing chat histories. <code>Redis</code> is often used as a fast, in-memory data store, commonly employed for caching, session management, and real-time data processing.</p> <p><pre><code>cd services/\ndocker compose up openllm  -d\n</code></pre> This service runs an OpenLLM (Language Model Server) service. It is configured with environment variables and GPU capabilities, used for serving large language models with support for GPU acceleration. Specific resource reservations and shared memory size are configured, optimized for running deep learning models. Additionally, it is mapped to volumes for cache and model storage, that serves language processing or machine learning tasks.</p> Run all services<pre><code>cd services/\ndocker compose up -d\n</code></pre> <p>Note</p> <p>Docker compose file was built with default persistence, it means that within the respective folders, database data is saved.</p>"},{"location":"get-started/quickstart/#manage-data","title":"Manage data","text":"<p>Introduces the concept of a Single Source of Truth (SSOT) for data ownership. It outlines uploading data and provides metadata formatting guidelines. Additionally, it explains the data ingestion process.</p>"},{"location":"get-started/quickstart/#single-source-of-truth","title":"Single source of truth","text":"<p>When a new data is uploaded, you should assign a Single Source of Truth (SSOT) to it. The SSOT is the owner of that data, and only the SSOT can modify or mutate it. The new data will be loaded into a <code>Mongo</code> database.</p>"},{"location":"get-started/quickstart/#upload-data","title":"Upload data","text":"<p>Partial path tree showing the organization of the documents directory</p> <pre><code>ICOS\n\u2514\u2500\u2500 documents\n    \u2514\u2500\u2500source\n        \u2514\u2500\u2500 language\n            \u251c\u2500\u2500 .loaded\n            \u2502   \u251c\u2500\u2500 doc_name.txt\n            \u2502   \u251c\u2500\u2500 ....\n            \u2502   \u2514\u2500\u2500 doc_name.pdf\n            \u251c\u2500\u2500 doc_name.pdf\n            \u251c\u2500\u2500 doc_name.pdf\n            \u2514\u2500\u2500 metadata\n                \u2514\u2500\u2500 document_metadata.json\n</code></pre> <p>Supported file formats</p> <ul> <li>PDF</li> <li>TXT</li> </ul>"},{"location":"get-started/quickstart/#metadata","title":"Metadata","text":"<p>Metadata should be uploaded to the metadata directory.  </p> <p>Json file of the metadata should reflect this formatting</p> <pre><code>{\n    \"document_name.txt\": {\n        \"attr\": \"value\",\n        \"attr\": \"value\"\n    },\n    \"document_name_2.txt\": {\n        \"attr\": \"value\",\n        \"attr\": \"value\"\n    },\n    ....\n}\n</code></pre> <p>The data must be uploaded to the appropriate Documents folder, as shown just above in the Partial path tree.</p> <p>To upload data to the server run the scp command:</p> command syntax<pre><code>scp [OPTION] [user@]SRC_HOST:]file1 [user@]DEST_HOST:]file2\n</code></pre> <p>After uploading data to server, how explain in section Single Source of Truth (SSOT) the data must be uploaded into a <code>Mongo</code> database.</p> <p>Upload data into a <code>Mongo</code> database:</p> <pre><code>python load_documents.py\n</code></pre> <p>Note</p> <p>Once the data is uploaded to Mongo it is moved to the subdirectory called <code>.loaded.</code>  This <code>.loaded</code> directory is useful for two reasons:</p> <pre><code>- Caching data already uploaded on Mongo\n- Persistence of uploaded data to server\n</code></pre>"},{"location":"get-started/quickstart/#ingest-data","title":"Ingest data","text":"<p>It loads documents from a specified source, processes them, extracts features using embeddings, and ingests them into a vector store.</p> <p>Ingest data:</p> <pre><code>python ingest.py\n</code></pre>"},{"location":"get-started/quickstart/#query-your-data","title":"Query your data","text":"<p>The core of the application resides in its chain which is exposed by the LangServe service.</p>"},{"location":"get-started/quickstart/#expose-the-chain","title":"Expose the chain","text":"<p><code>LangServe</code> sets up a FastAPI server. It loads various components such as embeddings, cross-encoder model, and callbacks. It then constructs a processing chain for language understanding and dialogue management. Finally, it adds routes for handling API requests related to chat functionality and runs the server using uvicorn on <code>localhost:8000</code>.</p> <p>Expose the chain: <pre><code>python run_server.py\n</code></pre></p>"},{"location":"get-started/quickstart/#run-client","title":"Run client","text":"<p>The client allows you to contact <code>LangServe</code> which provides the answer and associated context documents extracted from the chain.</p> <p>Run the following command: <pre><code>python run_client.py\n</code></pre></p>"},{"location":"get-started/quickstart/#trace-application","title":"Trace application","text":""},{"location":"get-started/quickstart/#langsmith","title":"LangSmith","text":"<p>LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application.</p> <p>A <code>Trace</code> is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an <code>LLM</code>, then to an output parser, and so on, all of these runs would be part of the same trace.</p>"},{"location":"guides/prompting/","title":"Prompting","text":"<p>This guide provide an overview of prompts and usage examples. The guide also includes tips, applications, limitations.</p>"},{"location":"guides/prompting/#prompt-engineering-guide-for-mixtral-8x7b","title":"Prompt Engineering Guide for Mixtral 8x7B","text":"<p>To effectively prompt the Mistral 8x7B Instruct and get optimal outputs, it's recommended to use the following chat template:</p> <pre><code>&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]\n</code></pre> <p>Note that <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.</p>"},{"location":"guides/prompting/#prompt-types","title":"Prompt types","text":"<p>The application has multiple prompts divided according to the task to be performed by the model. Main tasks that the model executes during chain execution are as follows:</p> <ul> <li>Standalone Question</li> <li>Self Query</li> <li>Question Answer</li> <li>Retry Output Parser</li> <li>Chat history summarization</li> </ul>"},{"location":"guides/prompting/#standalone-question","title":"Standalone Question","text":"<p>A standalone question in a RAG system improves context clarity and information retrieval, leading to more precise and relevant responses. It reduces ambiguity and enhances multi-turn dialogue performance. This is achieved by defining a sub-chain that reformulates the latest user question using historical messages if needed.</p> Prompt example<pre><code>[INST]\nGiven a chat history and the latest user question \nwhich might reference context in the chat history,\nformulate a standalone question which can be understood\nwithout the chat history. Do NOT answer the question,\njust reformulate it if needed and otherwise return it as is.\nConversation history:\n{history}\nQuestion: {question}\nStandalone question:\n[/INST]\n</code></pre>"},{"location":"guides/prompting/#self-query","title":"Self Query","text":"<p>A self-querying utilizes a query-constructing LLM chain to generate structured queries from natural language inputs. It applies these structured queries to its VectorStore, enabling it to perform semantic similarity comparisons and extract metadata filters from the user query to execute those filters on stored documents.</p> Prompt example<pre><code>[INST]\nYour goal is to structure the user's query to match the request schema provided below.\n\n&lt;&lt; Structured Request Schema &gt;&gt;\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\n\n```json\n{\n    \"query\": string \\ text string to compare to document contents\n    \"filter\": string \\ logical condition statement for filtering documents\n}\n\nProduce only a JSON object and avoid explaining or NOTE.\nThe query string should contain only text that is expected to match the contents of documents.\nAny conditions in the filter should not be mentioned in the query as well.\nA logical condition statement is composed of one or more comparison and logical operation statements.\n\nA comparison statement takes the form: `comp(attr, val)`:\n- `comp` ({allowed_comparators}): comparator\n- `attr` (string):  name of attribute to apply the comparison to\n- `val` (string): is the comparison value\n\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\n- `op` ({allowed_operators}): logical operator\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n\nMake sure that you only use the comparators and logical operators listed above and no others.\nMake sure that filters only refer to attributes that exist in the data source.\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n\n&lt;&lt; Data Source &gt;&gt;\n```json\n{{{{\n    \"content\": \"{content}\",\n    \"attributes\": {attributes}\n}}}}\n\n&lt;&lt; Example {i}. &gt;&gt;\nUser Query:\n{user_query}\n\nStructured Request:\n```json\n{structured_request}\n\n&lt;&lt; Example {i}. &gt;&gt;\nUser Query:\n{{query}}\n\nStructured Request:\n[/INST]\n</code></pre>"},{"location":"guides/prompting/#question-answer","title":"Question Answer","text":"<p>LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. </p> Prompt example<pre><code>[INST]\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved\ncontext to answer the question. If you don't know the answer, just say that you don't know.\nUse three sentences maximum and keep the answer concise.\nContext: {context}\nQuestion: {question}\nAnswer:\n[/INST]\n</code></pre>"},{"location":"guides/prompting/#chat-history-summarization","title":"Chat history summarization","text":"<p>Chat history summarization condenses previous interactions into a brief summary, capturing key points and context. This ensures the system can provide accurate and relevant responses in ongoing conversations, enhancing the overall user experience by maintaining coherence and avoiding information overload.</p> Prompt example<pre><code>[INST]\nProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full potential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n{summary}\n\nNew lines of conversation:\n{new_lines}\n\nNew summary:\n[/INST]\n</code></pre>"},{"location":"guides/prompting/#retry-output-parser","title":"Retry Output Parser","text":"<p>The retry output parser corrects errors in initial responses by analyzing the flaws and generating a revised output. It uses a debugging prompt template that includes the original prompt, flawed completion, and error details, ensuring the final response meets specified constraints and is accurate and consistent.</p> Prompt example<pre><code>&lt;s&gt;\nPrompt:\n{prompt}\nCompletion:\n{completion}\n&lt;/s&gt;\n[INST]\nAbove, the Completion did not satisfy the constraints given in the Prompt.\nDetails: {error}\nPlease try again:\n[/INST]\n</code></pre>"},{"location":"guides/prompting/#custom-prompt","title":"Custom prompt","text":"<p>Creating a custom prompt for a specific domain involves defining the system's behavior, instructions, metadata, and examples tailored to that domain. Here's a step-by-step guide to creating and using a custom prompt for a tourism assistant.</p> <p>Path tree showing the prompts package</p> <pre><code>.\n\u251c\u2500\u2500 domains\n\u2502   \u2514\u2500\u2500 tourism.py\n\u251c\u2500\u2500 qa_prompt.py\n\u251c\u2500\u2500 self_query_prompt.py\n\u251c\u2500\u2500 summarizer_prompt.py\n\u2514\u2500\u2500 standalone_prompt.py\n</code></pre>"},{"location":"guides/prompting/#steps-to-create-a-custom-prompt","title":"Steps to Create a Custom Prompt","text":"<ol> <li> <p>Define the System Prompt</p> <p>The system prompt sets the context and role of the assistant. For a tourism assistant, you might define it as follows:</p> <p><pre><code>TOURISM_ASSISTANT_SYSTEM_PROMPT = \"\"\"\n    You are a helpful tour assistant.\n    You will use only the provided context to answer tourist questions.\n    Read the given context before answering questions.\n    If you cannot answer a tourist question based on the provided context, inform the tourist.\n\"\"\"\n</code></pre> 2.  Define the System Instructions</p> <p>The instructions guide the assistant on how to use the context to answer questions. It includes placeholders for the context and the tourist's question:</p> <pre><code>TOURISM_ASSISTANT_SYSTEM_INSTRUCTION = \"\"\"\n    Context:\n    {context}\n    Tourist question: {question}\n    Answer:\n\"\"\"\n</code></pre> </li> <li> <p>Define Metadata Fields</p> <p>Metadata fields provide additional attributes that help filter and query documents more effectively. Each attribute is defined with a name, description, and type.</p> <p>Note</p> <p>TOURISM_METADATA_FIELD_INFO list provide a description of metadata related to uploaded documents. Read more here Manage data.</p> <pre><code>TOURISM_METADATA_FIELD_INFO = [\n    AttributeInfo(\n        name=\"document_name\",\n        description=\"The name of the document\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"source\",\n        description=\"The source of the document\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"language\",\n        description=\"The language of the document. One of ['it', 'en']\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"name\",\n        description=\"The name of the point of interest\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"city\",\n        description=\"The city where the point of interest is located\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"address\",\n        description=\"The address of the point of interest\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"type\",\n        description=\"The type of the point of interest\",\n        type=\"string\",\n    )\n]\n</code></pre> </li> <li> <p>Describe Document Content</p> <p>Provide a description of the type of content stored in the documents:</p> <pre><code>TOURISM_DOCUMENT_CONTENT_DESCRIPTION = \"Information related to points of interest in a city\"\n</code></pre> </li> <li> <p>Provide Examples</p> <p>Examples help the Self Query understand how to interpret and respond to different types of queries. Each example consists of a question, a query, and a filter:</p> <p>Tip</p> <p>Provide as many examples as possible and try generating with an artificial intelligence such as ChatGPT.</p> <pre><code>TOURISM_EXAMPLES = [\n    (\"What is the best gelato flavor in Florence?\",\n        {\n            \"query\": \"best gelato flavor\",\n            \"filter\": \"NO_FILTER\"\n        }\n    ),\n    (\"What are some popular tourist attractions in Rome?\",\n        {\n            \"query\": \"tourist attractions\",\n            \"filter\": \"eq(\\\"city\\\", \\\"Rome\\\")\"\n        }\n    ),\n    (\"Where can I find authentic Italian pizza in Naples?\",\n        {\n            \"query\": \"authentic Italian pizza\",\n            \"filter\": \"and(eq(\\\"city\\\", \\\"Naples\\\"), eq(\\\"type\\\", \\\"restaurant\\\"))\"\n        }\n    ),\n    (\"Show me museums in Paris or Berlin with at least 4 stars.\",\n        {\n            \"query\": \"museums\",\n            \"filter\": \"and(or(eq(\\\"city\\\", \\\"Paris\\\"), eq(\\\"city\\\", \\\"Berlin\\\")), gte(\\\"stars\\\", \\\"4\\\"))\"\n        }\n    ),\n    (\"I'm looking for a romantic gondola ride followed by a traditional seafood dinner in Venice.\",\n        {\n            \"query\": \"romantic gondola ride and seafood dinner\",\n            \"filter\": \"and(eq(\\\"city\\\", \\\"Venice\\\"), or(and(eq(\\\"type\\\", \\\"gondola ride\\\")), and(eq(\\\"type\\\", \\\"restaurant\\\"), eq(\\\"cuisine\\\", \\\"seafood\\\"), eq(\\\"ambiance\\\", \\\"romantic\\\"))))\"\n        }\n    )\n]\n</code></pre> </li> </ol>"},{"location":"guides/prompting/#implement-custom-domain-prompt","title":"Implement custom domain prompt","text":"<p>To use the custom domain prompt for a tourism assistant, you need to integrate the tourism-specific prompts and metadata into your system. This involves two main files <code>qa_prompt.py</code> and <code>self_query_prompt.py</code>.</p> qa_prompt.py<pre><code>from prompts.domains.tourism import (\n    TOURISM_ASSISTANT_SYSTEM_PROMPT,\n    TOURISM_ASSISTANT_SYSTEM_INSTRUCTION\n)\n\nBOS, EOS = '&lt;s&gt;', '&lt;/s&gt;'\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\nB_SYS, E_SYS = \"&lt;&lt;SYS&gt;&gt;\\n\", \"\\n&lt;&lt;/SYS&gt;&gt;\\n\\n\"\n\nLLAMA_QA_PROMPT_TEMPLATE = PromptTemplate.from_template(\n    template=B_INST + B_SYS + TOURISM_ASSISTANT_SYSTEM_PROMPT + E_SYS + TOURISM_ASSISTANT_SYSTEM_INSTRUCTION + E_INST\n)\n\n\nMIXTRAL_QA_PROMPT_TEMPLATE = PromptTemplate.from_template(\n    template=B_INST + TOURISM_ASSISTANT_SYSTEM_PROMPT + TOURISM_ASSISTANT_SYSTEM_INSTRUCTION + E_INST\n)\n</code></pre> self_query_prompt.py<pre><code>from prompts.domains.tourism import (\n    TOURISM_EXAMPLES,\n    TOURISM_METADATA_FIELD_INFO,\n    TOURISM_DOCUMENT_CONTENT_DESCRIPTION\n)\n\nEXAMPLES = TOURISM_EXAMPLES\n\nMETADATA_FIELD_INFO = TOURISM_METADATA_FIELD_INFO\n\nDOCUMENT_CONTENT_DESCRIPTION = TOURISM_DOCUMENT_CONTENT_DESCRIPTION\n</code></pre>"},{"location":"reference/ICOS/__init__/","title":"init","text":""},{"location":"reference/ICOS/__init__/#ICOS","title":"<code>ICOS</code>","text":"<p>My package.</p> <p>Modules:</p> Name Description <code>foo</code> <p>Description for <code>foo</code>.</p> <code>bar</code> <p>Description for <code>bar</code>.</p>"},{"location":"reference/ICOS/constants/","title":"Constants","text":""},{"location":"reference/ICOS/constants/#ICOS.constants","title":"<code>ICOS.constants</code>","text":""},{"location":"reference/ICOS/ingest/","title":"Ingest","text":""},{"location":"reference/ICOS/ingest/#ICOS.ingest","title":"<code>ICOS.ingest</code>","text":""},{"location":"reference/ICOS/ingest/#ICOS.ingest-functions","title":"Functions","text":""},{"location":"reference/ICOS/ingest/#ICOS.ingest.main","title":"<code>main(device, source, language, chunk_size, clear)</code>","text":"<p>This function loads documents from the specified source, processes them, extracts features using embeddings, and ingests them into a vector store. It provides options to select the device for computation, the source and  language of documents, metadata file names, chunk size for document processing, and an option to clear the  existing vector store collection.</p> <p>\b Parameters:     - device (str): Device to run on.     - source (str): Documents source.     - language (str): Documents language.     - metadata (str): Metadata file name.     - chunk_size (int): Size of document chunks.     - clear (bool): Flag to remove existing vector store collection.</p> <p>\b  Notes:      - For Mac user is recommended 'mps' device.</p>"},{"location":"reference/ICOS/load_documents/","title":"Load documents","text":""},{"location":"reference/ICOS/load_documents/#ICOS.load_documents","title":"<code>ICOS.load_documents</code>","text":""},{"location":"reference/ICOS/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/ICOS/preprocessing/#ICOS.preprocessing","title":"<code>ICOS.preprocessing</code>","text":""},{"location":"reference/ICOS/run_client/","title":"Run client","text":""},{"location":"reference/ICOS/run_client/#ICOS.run_client","title":"<code>ICOS.run_client</code>","text":""},{"location":"reference/ICOS/run_server/","title":"Run server","text":""},{"location":"reference/ICOS/run_server/#ICOS.run_server","title":"<code>ICOS.run_server</code>","text":""},{"location":"reference/ICOS/GUI/__init__/","title":"init","text":""},{"location":"reference/ICOS/GUI/__init__/#ICOS.GUI","title":"<code>ICOS.GUI</code>","text":""},{"location":"reference/ICOS/GUI/main_page/","title":"Main page","text":""},{"location":"reference/ICOS/GUI/main_page/#ICOS.GUI.main_page","title":"<code>ICOS.GUI.main_page</code>","text":""},{"location":"reference/ICOS/GUI/upload/","title":"Upload","text":""},{"location":"reference/ICOS/GUI/upload/#ICOS.GUI.upload","title":"<code>ICOS.GUI.upload</code>","text":""},{"location":"reference/ICOS/callbacks/__init__/","title":"init","text":""},{"location":"reference/ICOS/callbacks/__init__/#ICOS.callbacks","title":"<code>ICOS.callbacks</code>","text":""},{"location":"reference/ICOS/callbacks/handler/","title":"Handler","text":""},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler","title":"<code>ICOS.callbacks.handler</code>","text":""},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler-classes","title":"Classes","text":""},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler","title":"<code>CallBackHandler(show_sources: bool = True)</code>","text":"<p>               Bases: <code>BaseCallbackHandler</code></p> <p>A callback handler for various events during model execution.</p> <p>Methods:</p> Name Description <code>on_llm_start</code> <p>Run when LLM starts running.</p> <code>on_chat_model_start</code> <p>Run when Chat Model starts running.</p> <code>on_llm_new_token</code> <p>Run on new LLM token. Only available when streaming is enabled.</p> <code>on_llm_end</code> <p>Run when LLM ends running.</p> <code>on_llm_error</code> <p>Run when LLM errors.</p> <code>on_chain_start</code> <p>Run when chain starts running.</p> <code>on_chain_end</code> <p>Run when chain ends running.</p> <code>on_chain_error</code> <p>Run when chain errors.</p> <code>on_tool_start</code> <p>Run when tool starts running.</p> <code>on_tool_end</code> <p>Run when tool ends running.</p> <code>on_tool_error</code> <p>Run when tool errors.</p> <code>on_text</code> <p>Run on arbitrary text.</p> <code>on_agent_action</code> <p>Run on agent action.</p> <code>on_agent_finish</code> <p>Run on agent end.</p> <p>Initialize CallBackHandler with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>show_sources</code> <code>bool</code> <p>Flag indicating if source documents should be displayed. Defaults to True.</p> <code>True</code>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler-functions","title":"Functions","text":""},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_agent_action","title":"<code>on_agent_action(action: AgentAction, **kwargs: Any) -&gt; Any</code>","text":"<p>Run on agent action.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_agent_finish","title":"<code>on_agent_finish(finish: AgentFinish, **kwargs: Any) -&gt; Any</code>","text":"<p>Run on agent end.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_chain_end","title":"<code>on_chain_end(outputs: Dict[str, Any], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when chain ends running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_chain_error","title":"<code>on_chain_error(error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when chain errors.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_chain_start","title":"<code>on_chain_start(serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when chain starts running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_chat_model_start","title":"<code>on_chat_model_start(serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when Chat Model starts running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_llm_end","title":"<code>on_llm_end(response: LLMResult, **kwargs: Any) -&gt; Any</code>","text":"<p>Run when LLM ends running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_llm_error","title":"<code>on_llm_error(error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when LLM errors.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_llm_new_token","title":"<code>on_llm_new_token(token: str, **kwargs: Any) -&gt; Any</code>","text":"<p>Run on new LLM token. Only available when streaming is enabled.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_llm_start","title":"<code>on_llm_start(serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when LLM starts running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_text","title":"<code>on_text(text: str, **kwargs: Any) -&gt; Any</code>","text":"<p>Run on arbitrary text.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_tool_end","title":"<code>on_tool_end(output: str, **kwargs: Any) -&gt; Any</code>","text":"<p>Run when tool ends running.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_tool_error","title":"<code>on_tool_error(error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -&gt; Any</code>","text":"<p>Run when tool errors.</p>"},{"location":"reference/ICOS/callbacks/handler/#ICOS.callbacks.handler.CallBackHandler.on_tool_start","title":"<code>on_tool_start(serialized: Dict[str, Any], input_str: str, **kwargs: Any) -&gt; Any</code>","text":"<p>Run when tool starts running.</p>"},{"location":"reference/ICOS/chains/__init__/","title":"init","text":""},{"location":"reference/ICOS/chains/__init__/#ICOS.chains","title":"<code>ICOS.chains</code>","text":""},{"location":"reference/ICOS/chains/base/","title":"Base","text":""},{"location":"reference/ICOS/chains/base/#ICOS.chains.base","title":"<code>ICOS.chains.base</code>","text":""},{"location":"reference/ICOS/chains/base/#ICOS.chains.base-classes","title":"Classes","text":""},{"location":"reference/ICOS/chains/base/#ICOS.chains.base.Chain","title":"<code>Chain(device: str = 'cuda', llm: BaseLanguageModel = None, retriever: BaseRetriever = None, vectorstore: Chroma = None, session_id: BaseChatMessageHistory = None, cross_encoder: HuggingFaceCrossEncoder = None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Represents a chain of language processing components.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to run the chain on. Defaults to 'cuda'.</p> <code>'cuda'</code> <code>llm</code> <code>BaseLanguageModel</code> <p>The language model to use in the chain.</p> <code>None</code> <code>retriever</code> <code>BaseRetriever</code> <p>The retriever component for fetching documents.</p> <code>None</code> <code>vectorstore</code> <code>Chroma</code> <p>The vector store component for encoding documents.</p> <code>None</code> <code>session_id</code> <code>BaseChatMessageHistory</code> <p>The session identifier for tracking conversations.</p> <code>None</code> <code>cross_encoder</code> <code>HuggingFaceCrossEncoder</code> <p>The cross encoder model for re-ranking retrieved documents.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>device</code> <code>str</code> <p>The device to run the chain on.</p> <code>llm</code> <code>BaseLanguageModel</code> <p>The language model used in the chain.</p> <code>retriever</code> <code>BaseRetriever</code> <p>The retriever component for fetching documents.</p> <code>vectorstore</code> <code>Chroma</code> <p>The vector store component for encoding documents.</p> <code>session_id</code> <code>BaseChatMessageHistory</code> <p>The session identifier for tracking conversations.</p> <code>cross_encoder</code> <code>HuggingFaceCrossEncoder</code> <p>The cross encoder model for re-ranking retrieved documents.</p> <p>Methods:</p> Name Description <code>get_lcel_chain</code> <p>Generates a language chain for standalone questions with history in buffer.</p> <code>get_lcel_with_history_chain</code> <p>Generates a language chain for questions with persistent history on database.</p>"},{"location":"reference/ICOS/chains/base/#ICOS.chains.base.Chain-functions","title":"Functions","text":""},{"location":"reference/ICOS/chains/base/#ICOS.chains.base.Chain.get_runnable_chain","title":"<code>get_runnable_chain()</code>","text":"<p>Generates a language chain for questions with persistent history on database.</p> <p>Returns:</p> Name Type Description <code>CustomRunnableWithMessageHistory</code> <p>A runnable language chain with message history support.</p>"},{"location":"reference/ICOS/chains/self_query_constructor/","title":"Self query constructor","text":""},{"location":"reference/ICOS/chains/self_query_constructor/#ICOS.chains.self_query_constructor","title":"<code>ICOS.chains.self_query_constructor</code>","text":""},{"location":"reference/ICOS/chains/self_query_constructor/#ICOS.chains.self_query_constructor-functions","title":"Functions","text":""},{"location":"reference/ICOS/chains/self_query_constructor/#ICOS.chains.self_query_constructor.construct_examples","title":"<code>construct_examples(input_output_pairs: Sequence[Tuple[str, dict]]) -&gt; List[dict]</code>","text":"<p>Construct examples from input-output pairs.</p> <p>Parameters:</p> Name Type Description Default <code>input_output_pairs</code> <code>Sequence[Tuple[str, dict]]</code> <p>Sequence of input-output pairs.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of examples.</p>"},{"location":"reference/ICOS/chains/self_query_constructor/#ICOS.chains.self_query_constructor.get_query_constructor_prompt","title":"<code>get_query_constructor_prompt(allowed_comparators: Sequence[Comparator] = tuple(Comparator), allowed_operators: Sequence[Operator] = tuple(Operator), **kwargs: Any) -&gt; BasePromptTemplate</code>","text":"<p>Create query construction prompt.</p> <p>Parameters:</p> Name Type Description Default <code>document_contents</code> <p>The contents of the document to be queried.</p> required <code>attribute_info</code> <p>A list of AttributeInfo objects describing the attributes of the document.</p> required <code>examples</code> <p>Optional list of examples to use for the chain.</p> required <code>allowed_comparators</code> <code>Sequence[Comparator]</code> <p>Sequence of allowed comparators.</p> <code>tuple(Comparator)</code> <code>allowed_operators</code> <code>Sequence[Operator]</code> <p>Sequence of allowed operators.</p> <code>tuple(Operator)</code> <code>enable_limit</code> <p>Whether to enable the limit operator. Defaults to False.</p> required <code>schema_prompt</code> <p>Prompt for describing query schema. Should have string input variables allowed_comparators and allowed_operators.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional named params to pass to FewShotPromptTemplate init.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BasePromptTemplate</code> <p>A prompt template that can be used to construct queries.</p>"},{"location":"reference/ICOS/chains/self_query_constructor/#ICOS.chains.self_query_constructor.load_query_constructor_runnable","title":"<code>load_query_constructor_runnable(llm: BaseLanguageModel, *, allowed_comparators: Sequence[Comparator] = tuple(Comparator), allowed_operators: Sequence[Operator] = tuple(Operator), fix_invalid: bool = False, **kwargs: Any) -&gt; Runnable</code>","text":"<p>Load a query constructor runnable chain.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLanguageModel</code> <p>BaseLanguageModel to use for the chain.</p> required <code>document_contents</code> <p>Description of the page contents of the document to be queried.</p> required <code>attribute_info</code> <p>Sequence of attributes in the document.</p> required <code>examples</code> <p>Optional list of examples to use for the chain.</p> required <code>allowed_comparators</code> <code>Sequence[Comparator]</code> <p>Sequence of allowed comparators. Defaults to all Comparators.</p> <code>tuple(Comparator)</code> <code>allowed_operators</code> <code>Sequence[Operator]</code> <p>Sequence of allowed operators. Defaults to all Operators.</p> <code>tuple(Operator)</code> <code>enable_limit</code> <p>Whether to enable the limit operator. Defaults to False.</p> required <code>schema_prompt</code> <p>Prompt for describing query schema. Should have string input variables allowed_comparators and allowed_operators.</p> required <code>fix_invalid</code> <code>bool</code> <p>Whether to fix invalid filter directives by ignoring invalid operators, comparators and attributes.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional named params to pass to FewShotPromptTemplate init.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Runnable</code> <p>A Runnable that can be used to construct queries.</p>"},{"location":"reference/ICOS/llms/__init__/","title":"init","text":""},{"location":"reference/ICOS/llms/__init__/#ICOS.llms","title":"<code>ICOS.llms</code>","text":""},{"location":"reference/ICOS/llms/clients/__init__/","title":"init","text":""},{"location":"reference/ICOS/llms/clients/__init__/#ICOS.llms.clients","title":"<code>ICOS.llms.clients</code>","text":""},{"location":"reference/ICOS/llms/clients/openllm/","title":"Openllm","text":""},{"location":"reference/ICOS/llms/clients/openllm/#ICOS.llms.clients.openllm","title":"<code>ICOS.llms.clients.openllm</code>","text":""},{"location":"reference/ICOS/loaders/__init__/","title":"init","text":""},{"location":"reference/ICOS/loaders/__init__/#ICOS.loaders","title":"<code>ICOS.loaders</code>","text":""},{"location":"reference/ICOS/loaders/callbacks/","title":"Callbacks","text":""},{"location":"reference/ICOS/loaders/callbacks/#ICOS.loaders.callbacks","title":"<code>ICOS.loaders.callbacks</code>","text":""},{"location":"reference/ICOS/loaders/callbacks/#ICOS.loaders.callbacks-functions","title":"Functions","text":""},{"location":"reference/ICOS/loaders/callbacks/#ICOS.loaders.callbacks.load_langfuse","title":"<code>load_langfuse() -&gt; Dict[str, Any]</code>","text":"<p>Load LangFuse callback handlers for LangChain model execution.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing LangFuse callback handlers.</p>"},{"location":"reference/ICOS/loaders/callbacks/#ICOS.loaders.callbacks.load_langsmith","title":"<code>load_langsmith() -&gt; Dict[str, Any]</code>","text":"<p>Load LangSmith callback handlers for LangChain model execution.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing LangSmith callback handlers.</p>"},{"location":"reference/ICOS/loaders/documents/","title":"Documents","text":""},{"location":"reference/ICOS/loaders/documents/#ICOS.loaders.documents","title":"<code>ICOS.loaders.documents</code>","text":""},{"location":"reference/ICOS/loaders/documents/#ICOS.loaders.documents-classes","title":"Classes","text":""},{"location":"reference/ICOS/loaders/documents/#ICOS.loaders.documents.CustomDocument","title":"<code>CustomDocument</code>","text":"<p>               Bases: <code>Document</code></p> <p>Class for storing a piece of text and associated metadata.</p>"},{"location":"reference/ICOS/loaders/embedding/","title":"Embedding","text":""},{"location":"reference/ICOS/loaders/embedding/#ICOS.loaders.embedding","title":"<code>ICOS.loaders.embedding</code>","text":""},{"location":"reference/ICOS/loaders/embedding/#ICOS.loaders.embedding-classes","title":"Classes","text":""},{"location":"reference/ICOS/loaders/embedding/#ICOS.loaders.embedding.EmbeddingLoader","title":"<code>EmbeddingLoader(device: str = 'cuda', model_name: str = EMBEDDING_MODEL_NAME, cache_folder: str = SENTENCE_TRANSFORMERS_HOME)</code>","text":"<p>               Bases: <code>object</code></p> <p>Loads embeddings models from Hugging Face.</p> <p>Attributes:</p> Name Type Description <code>_DEFAULT_DEVICE</code> <code>str</code> <p>Default device for loading embeddings.</p> <code>_DEFAULT_MODEL_NAME</code> <code>str</code> <p>Default name of the embedding model.</p> <code>_DEFAULT_CACHE_FOLDER</code> <code>str</code> <p>Default folder for caching models.</p> <p>Initializes an EmbeddingLoader object.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device for loading embeddings. Default is 'cuda'.</p> <code>'cuda'</code> <code>model_name</code> <code>str</code> <p>Name of the embedding model. Default is the value of EMBEDDING_MODEL_NAME.</p> <code>EMBEDDING_MODEL_NAME</code> <code>cache_folder</code> <code>str</code> <p>Folder for caching models. Default is the 'models' subdirectory of ROOT_DIRECTORY.</p> <code>SENTENCE_TRANSFORMERS_HOME</code>"},{"location":"reference/ICOS/loaders/embedding/#ICOS.loaders.embedding.EmbeddingLoader-functions","title":"Functions","text":""},{"location":"reference/ICOS/loaders/embedding/#ICOS.loaders.embedding.EmbeddingLoader.load","title":"<code>load() -&gt; Union[HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings]</code>","text":"<p>Load embeddings model.</p> <p>Returns:</p> Type Description <code>Union[HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings]</code> <p>Union[HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings]: Instance of HuggingFaceEmbeddings or HuggingFaceInstructEmbeddings.</p>"},{"location":"reference/ICOS/loaders/local/","title":"Local","text":""},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local","title":"<code>ICOS.loaders.local</code>","text":""},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local-classes","title":"Classes","text":""},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local.DocumentLoader","title":"<code>DocumentLoader(source: str = None, language: str = None, metadata_file_name: str = None, documents_path: str = join(ROOT_DIRECTORY, 'documents'), db_documents: BaseLoader = None)</code>","text":"<p>               Bases: <code>BaseLoader</code></p>"},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local.DocumentLoader-functions","title":"Functions","text":""},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local.DocumentLoader.load","title":"<code>load() -&gt; List[Document]</code>","text":"<p>Read all files in a directory</p>"},{"location":"reference/ICOS/loaders/local/#ICOS.loaders.local.DocumentLoader.load_file","title":"<code>load_file(file_path: str) -&gt; Document | None</code>","text":"<p>Read a file</p>"},{"location":"reference/ICOS/loaders/model/","title":"Model","text":""},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model","title":"<code>ICOS.loaders.model</code>","text":""},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model-classes","title":"Classes","text":""},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model.ModelLoader","title":"<code>ModelLoader(model_id: str = 'TheBloke/Llama-2-7b-Chat-GGUF', model_basename: str = 'llama-2-7b-chat.Q4_K_M.gguf', models_path: str = join(ROOT_DIRECTORY, 'models'), context_window_size: int = 2048, max_new_tokens: int = 2048, n_batch: int = 512, n_gpu_layers: int = 100, temperature: float = 0.2)</code>","text":"<p>               Bases: <code>object</code></p> <p>A class for loading various large language models (LLMs).</p> <p>This class provides methods to load different LLM implementations, including LlamaCpp, VLLM, and OpenLLM. It allows for flexibility in choosing the appropriate LLM based on factors like performance requirements and available resources.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The ID of the LLM model to be loaded (e.g., from the Hugging Face Hub).             Defaults to \"TheBloke/Llama-2-7b-Chat-GGUF\". (Commented out options provided for reference)</p> <code>model_basename</code> <code>str</code> <p>The base filename of the LLM model (e.g., \"llama-2-7b-chat.Q4_K_M.gguf\").                  Defaults to \"llama-2-7b-chat.Q4_K_M.gguf\". (Commented out options provided for reference)</p> <code>models_path</code> <code>str</code> <p>The path to the directory where LLM models are stored.                Defaults to joining the <code>ROOT_DIRECTORY</code> constant with \"models\".</p> <code>context_window_size</code> <code>int</code> <p>The size of the context window used by the LLM for processing input sequences.                        Defaults to 2048. (Commented out options provided for reference)</p> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of new tokens the LLM can generate in a response.                  Defaults to 2048. (Commented out options provided for reference)</p> <code>n_batch</code> <code>int</code> <p>The number of samples to process in a batch during LLM inference.            Defaults to 512.</p> <code>n_gpu_layers</code> <code>int</code> <p>The number of GPU layers to use for LLM inference.                Defaults to 100. (Previously had a commented-out option of 1)</p> <code>temperature</code> <code>float</code> <p>A temperature parameter controlling the randomness of LLM outputs.                   Higher values lead to more diverse but potentially less coherent responses.                   Defaults to 0.2.</p>"},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model.ModelLoader-functions","title":"Functions","text":""},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model.ModelLoader.get_model_path","title":"<code>get_model_path()</code>","text":"<p>Downloads and retrieves the path to the specified LLM model.</p> <p>This method utilizes the <code>hf_hub_download</code> function (likely from the Hugging Face Transformers library) to download the model from the Hugging Face Hub if it's not already present in the configured <code>models_path</code>. It then returns the path to the downloaded or existing model file.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the downloaded or existing LLM model file.</p>"},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model.ModelLoader.load_llamacpp","title":"<code>load_llamacpp() -&gt; LlamaCpp</code>","text":"<p>Loads an LLM using the LlamaCpp library.</p> <p>This method creates an instance of the <code>LlamaCpp</code> class (likely from a separate LlamaCpp library), configuring it to use the specified LLM model and parameters for inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>ModelLoader</code> <p>The current <code>ModelLoader</code> instance.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>LlamaCpp</code> <p>An instance of the <code>LlamaCpp</code> class, ready for use.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>LlamaCpp</code> library is not installed.</p>"},{"location":"reference/ICOS/loaders/model/#ICOS.loaders.model.ModelLoader.load_vllm","title":"<code>load_vllm() -&gt; VLLM</code>","text":"<p>Loads an LLM using the VLLM library.</p> <p>This method likely creates an instance of the <code>VLLM</code> class (potentially from a separate VLLM library). It provides the following arguments:</p> <ul> <li>model (str): The ID of the LLM model to be loaded from the Hugging Face Hub                  (e.g., \"TheBloke/Llama-2-7b-Chat-AWQ\").</li> <li>download_dir (str): The directory where the downloaded model will be stored                         (defaults to \"/mt/vdc1/home/llm-project/ICOS/models\").</li> <li>trust_remote_code (bool): Whether to trust remotely hosted model code                                (use with caution, security implications might exist).</li> <li>max_new_tokens (int): The maximum number of new tokens the LLM can generate in a response                          (set to the class's default value, 2048).</li> <li>vllm_kwargs (dict): Additional keyword arguments specific to the VLLM library configuration.                      Here, it sets the quantization scheme to \"awq\" (potentially affecting performance and memory usage)                      and the GPU memory utilization target to 0.7 (a value between 0 and 1).</li> <li>temperature (float): The temperature parameter controlling the randomness of LLM outputs                           (set to the class's default value, 0.2).</li> <li>verbose (bool): Whether to print verbose logging information during the loading process (defaults to True).</li> </ul> <p>Returns:</p> Name Type Description <code>object</code> <code>VLLM</code> <p>An instance of the <code>VLLM</code> class, ready for use.</p>"},{"location":"reference/ICOS/loaders/remote/","title":"Remote","text":""},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote","title":"<code>ICOS.loaders.remote</code>","text":""},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote-classes","title":"Classes","text":""},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote.MongodbClient","title":"<code>MongodbClient(connection_string: str = MONGO_DOCS_CONN_STRING, db_name: str = MONGO_DOCS_DB_NAME, collection_name: str = MONGO_DOCS_COLLECTION, filter_criteria: Optional[Dict] = None)</code>","text":"<p>               Bases: <code>MongodbLoader</code></p> <p>Client for interacting with MongoDB.</p> <p>Initialize the MongoDB clients.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>MongoDB connection string.</p> <code>MONGO_DOCS_CONN_STRING</code> <code>db_name</code> <code>str</code> <p>Database name.</p> <code>MONGO_DOCS_DB_NAME</code> <code>collection_name</code> <code>str</code> <p>Collection name.</p> <code>MONGO_DOCS_COLLECTION</code> <code>filter_criteria</code> <code>Optional[Dict]</code> <p>Filter criteria for querying documents.</p> <code>None</code>"},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote.MongodbClient-functions","title":"Functions","text":""},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote.MongodbClient.aload","title":"<code>aload() -&gt; List[Document]</code>  <code>async</code>","text":"<p>Asynchronously load documents from MongoDB.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents.</p>"},{"location":"reference/ICOS/loaders/remote/#ICOS.loaders.remote.MongodbClient.insert","title":"<code>insert(documents: List[Document])</code>","text":"<p>Insert documents into MongoDB.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source of the documents.</p> required <code>documents</code> <code>List[Document]</code> <p>....</p> required"},{"location":"reference/ICOS/prompts/__init__/","title":"init","text":""},{"location":"reference/ICOS/prompts/__init__/#ICOS.prompts","title":"<code>ICOS.prompts</code>","text":""},{"location":"reference/ICOS/prompts/assistant_prompt/","title":"Assistant prompt","text":""},{"location":"reference/ICOS/prompts/assistant_prompt/#ICOS.prompts.assistant_prompt","title":"<code>ICOS.prompts.assistant_prompt</code>","text":""},{"location":"reference/ICOS/prompts/qa_prompt/","title":"Qa prompt","text":""},{"location":"reference/ICOS/prompts/qa_prompt/#ICOS.prompts.qa_prompt","title":"<code>ICOS.prompts.qa_prompt</code>","text":""},{"location":"reference/ICOS/prompts/self_query_prompt/","title":"Self query prompt","text":""},{"location":"reference/ICOS/prompts/self_query_prompt/#ICOS.prompts.self_query_prompt","title":"<code>ICOS.prompts.self_query_prompt</code>","text":""},{"location":"reference/ICOS/prompts/standalone_prompt/","title":"Standalone prompt","text":""},{"location":"reference/ICOS/prompts/standalone_prompt/#ICOS.prompts.standalone_prompt","title":"<code>ICOS.prompts.standalone_prompt</code>","text":""},{"location":"reference/ICOS/runnables/__init__/","title":"init","text":""},{"location":"reference/ICOS/runnables/__init__/#ICOS.runnables","title":"<code>ICOS.runnables</code>","text":""},{"location":"reference/ICOS/runnables/history/","title":"History","text":""},{"location":"reference/ICOS/runnables/history/#ICOS.runnables.history","title":"<code>ICOS.runnables.history</code>","text":""},{"location":"reference/ICOS/runnables/history/#ICOS.runnables.history-classes","title":"Classes","text":""},{"location":"reference/ICOS/runnables/history/#ICOS.runnables.history.CustomRunnableWithMessageHistory","title":"<code>CustomRunnableWithMessageHistory</code>","text":"<p>               Bases: <code>RunnableWithMessageHistory</code></p> <p>Custom implementation of RunnableWithMessageHistory.</p>"},{"location":"reference/ICOS/server/__init__/","title":"init","text":""},{"location":"reference/ICOS/server/__init__/#ICOS.server","title":"<code>ICOS.server</code>","text":""},{"location":"reference/ICOS/server/langserve/","title":"Langserve","text":""},{"location":"reference/ICOS/server/langserve/#ICOS.server.langserve","title":"<code>ICOS.server.langserve</code>","text":""},{"location":"reference/ICOS/server/session/","title":"Session","text":""},{"location":"reference/ICOS/server/session/#ICOS.server.session","title":"<code>ICOS.server.session</code>","text":""},{"location":"reference/ICOS/server/session/#ICOS.server.session-functions","title":"Functions","text":""},{"location":"reference/ICOS/server/session/#ICOS.server.session.create_session_factory","title":"<code>create_session_factory(base_url: Union[str] = 'redis://127.0.0.1:6379/0') -&gt; Callable[[str], BaseChatMessageHistory]</code>","text":"<p>Create a factory that can retrieve chat histories.</p> <p>The chat histories are keyed by user ID and conversation ID.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>Union[str]</code> <p>Base url to use for storing the chat histories in Redis.</p> <code>'redis://127.0.0.1:6379/0'</code> <p>Returns:</p> Name Type Description <code>RedisChatMessageHistory</code> <code>Callable[[str], BaseChatMessageHistory]</code> <p>A factory function that takes a session ID as input                       and returns a chat history object.</p>"},{"location":"reference/ICOS/server/session/#ICOS.server.session.is_valid_identifier","title":"<code>is_valid_identifier(value: str) -&gt; bool</code>","text":"<p>Checks if the provided string <code>value</code> is a valid identifier according to Python's naming conventions.</p> <p>An identifier can start with a letter (uppercase or lowercase) or an underscore (<code>_</code>), followed by letters, numbers, or underscores.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to be checked for validity as an identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if <code>value</code> is a valid identifier, False otherwise.</p>"},{"location":"reference/ICOS/server/session/#ICOS.server.session.per_request_config_modifier","title":"<code>per_request_config_modifier(config: Dict[str, Any], request: Request) -&gt; Dict[str, Any]</code>","text":"<p>Modifies a configuration dictionary based on a request object (potentially used for per-request configuration).</p> <p>This function appears to be incomplete and might not be fully functional. It's commented out as it's not directly related to session ID validation or chat history retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to be modified.</p> required <code>request</code> <code>Request</code> <p>The request object (potentially containing cookies).</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The modified configuration dictionary.</p>"},{"location":"reference/ICOS/text_splitters/__init__/","title":"init","text":""},{"location":"reference/ICOS/text_splitters/__init__/#ICOS.text_splitters","title":"<code>ICOS.text_splitters</code>","text":""},{"location":"reference/ICOS/text_splitters/chunker/","title":"Chunker","text":""},{"location":"reference/ICOS/text_splitters/chunker/#ICOS.text_splitters.chunker","title":"<code>ICOS.text_splitters.chunker</code>","text":""},{"location":"reference/ICOS/text_splitters/chunker/#ICOS.text_splitters.chunker-classes","title":"Classes","text":""},{"location":"reference/ICOS/text_splitters/chunker/#ICOS.text_splitters.chunker.Chunker","title":"<code>Chunker(chunk_size: int = 1024, chunk_overlap: int = 128, length_function=len, is_separator_regex=False)</code>","text":"<p>               Bases: <code>object</code></p> <p>Splits text into chunks for processing.</p> <p>Attributes:</p> Name Type Description <code>_DEFAULT_CHUNK_SIZE</code> <code>int</code> <p>Default size of each chunk.</p> <code>_DEFAULT_CHUNK_OVERLAP</code> <code>int</code> <p>Default overlap between chunks.</p> <p>Methods:</p> Name Description <code>get_recursive_character_text_splitter</code> <p>Get a RecursiveCharacterTextSplitter instance with current settings.</p> <p>Initializes a Chunker object.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Size of each chunk. Default is 1024.</p> <code>1024</code> <code>chunk_overlap</code> <code>int</code> <p>Overlap between chunks. Default is 128.</p> <code>128</code> <code>length_function</code> <code>callable</code> <p>Function to calculate the length of text.</p> <code>len</code> <code>is_separator_regex</code> <code>bool</code> <p>Whether the separator is a regular expression. Default is False.</p> <code>False</code>"},{"location":"reference/ICOS/text_splitters/chunker/#ICOS.text_splitters.chunker.Chunker-functions","title":"Functions","text":""},{"location":"reference/ICOS/text_splitters/chunker/#ICOS.text_splitters.chunker.Chunker.get_recursive_character_text_splitter","title":"<code>get_recursive_character_text_splitter() -&gt; RecursiveCharacterTextSplitter</code>","text":"<p>Get a RecursiveCharacterTextSplitter instance with current settings.</p> <p>Returns:</p> Name Type Description <code>RecursiveCharacterTextSplitter</code> <code>RecursiveCharacterTextSplitter</code> <p>A RecursiveCharacterTextSplitter instance.</p>"},{"location":"reference/ICOS/utils/__init__/","title":"init","text":""},{"location":"reference/ICOS/utils/__init__/#ICOS.utils","title":"<code>ICOS.utils</code>","text":""},{"location":"reference/ICOS/utils/converter/","title":"Converter","text":""},{"location":"reference/ICOS/utils/converter/#ICOS.utils.converter","title":"<code>ICOS.utils.converter</code>","text":""},{"location":"reference/ICOS/utils/converter/#ICOS.utils.converter-functions","title":"Functions","text":""},{"location":"reference/ICOS/utils/converter/#ICOS.utils.converter.to_documents","title":"<code>to_documents(documents: str) -&gt; List[Document]</code>","text":"<p>Converts documents from JSON format to a list of Document object.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document in JSON format.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of documents: A List of Document object.</p>"},{"location":"reference/ICOS/utils/converter/#ICOS.utils.converter.to_json","title":"<code>to_json(documents: List[Document]) -&gt; str</code>","text":"<p>Converts documents to JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON representation of the documents.</p>"},{"location":"reference/ICOS/utils/uploader/","title":"Uploader","text":""},{"location":"reference/ICOS/utils/uploader/#ICOS.utils.uploader","title":"<code>ICOS.utils.uploader</code>","text":""},{"location":"reference/ICOS/utils/uploader/#ICOS.utils.uploader-functions","title":"Functions","text":""},{"location":"reference/ICOS/utils/uploader/#ICOS.utils.uploader.upload","title":"<code>upload(src: str, dst_src_name: str, dst_src_lang: str) -&gt; None</code>","text":"<p>Copy files from source directory or file to destination directory, overwriting existing files in destination only if source files are newer.</p>"},{"location":"reference/ICOS/utils/uploader/#ICOS.utils.uploader.upload_file","title":"<code>upload_file(src, dst)</code>","text":"<p>Copy a file from source to destination, overwriting existing file in destination only if source file is newer.</p>"},{"location":"reference/ICOS/utils/wikipedia/","title":"Wikipedia","text":""},{"location":"reference/ICOS/utils/wikipedia/#ICOS.utils.wikipedia","title":"<code>ICOS.utils.wikipedia</code>","text":""},{"location":"reference/ICOS/utils/wikipedia/#ICOS.utils.wikipedia-functions","title":"Functions","text":""},{"location":"reference/ICOS/utils/wikipedia/#ICOS.utils.wikipedia.download_wikipedia_page","title":"<code>download_wikipedia_page(title: str, language: str = 'it') -&gt; None</code>","text":"<p>Load a Wikipedia page in system directory document. If eng_version is True, you should upload its english version.</p>"},{"location":"reference/ICOS/utils/wikipedia/#ICOS.utils.wikipedia.read_points_of_interest","title":"<code>read_points_of_interest() -&gt; List[str]</code>","text":"<p>Read points of interest</p>"},{"location":"reference/ICOS/vector_stores/__init__/","title":"init","text":""},{"location":"reference/ICOS/vector_stores/__init__/#ICOS.vector_stores","title":"<code>ICOS.vector_stores</code>","text":""},{"location":"reference/ICOS/vector_stores/chroma/","title":"Chroma","text":""},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma","title":"<code>ICOS.vector_stores.chroma</code>","text":""},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma-classes","title":"Classes","text":""},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma.ChromaLoader","title":"<code>ChromaLoader(collection_name: str = _DEFAULT_COLLECTION_NAME, persist_directory: str = join(ROOT_DIRECTORY, 'chroma'), client_settings: Optional[chromadb.config.Settings] = Settings(anonymized_telemetry=False, is_persistent=True), embedding: Optional[Embeddings] = None)</code>","text":"<p>               Bases: <code>object</code></p> <p>A class for loading Chroma vector store and interacting with it.</p> <p>Attributes:</p> Name Type Description <code>_DEFAULT_COLLECTION_NAME</code> <code>str</code> <p>Default name for the collection.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load Chroma vector store with configured parameters.</p> <code>add_documents</code> <p>Add documents to the Chroma vector store.</p> <code>delete</code> <p>Delete the Chroma vector store directory if it exists.</p> <p>Initialize ChromaLoader with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection. Defaults to _DEFAULT_COLLECTION_NAME.</p> <code>_DEFAULT_COLLECTION_NAME</code> <code>persist_directory</code> <code>str</code> <p>Directory path to persist Chroma data. Defaults to join(ROOT_DIRECTORY, 'chroma').</p> <code>join(ROOT_DIRECTORY, 'chroma')</code> <code>client_settings</code> <code>Optional[Settings]</code> <p>Chromadb clients settings. Defaults to None.</p> <code>Settings(anonymized_telemetry=False, is_persistent=True)</code> <code>embedding</code> <code>Optional[Embeddings]</code> <p>Embedding model. Defaults to None.</p> <code>None</code>"},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma.ChromaLoader-functions","title":"Functions","text":""},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma.ChromaLoader.add_documents","title":"<code>add_documents(documents: list[Document]) -&gt; None</code>","text":"<p>Add documents to the Chroma vector store.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Document]</code> <p>List of documents to add.</p> required"},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma.ChromaLoader.delete","title":"<code>delete() -&gt; None</code>","text":"<p>Delete the Chroma vector store directory if it exists.</p>"},{"location":"reference/ICOS/vector_stores/chroma/#ICOS.vector_stores.chroma.ChromaLoader.load","title":"<code>load() -&gt; Chroma</code>","text":"<p>Load Chroma vector store with configured parameters.</p> <p>Returns:</p> Name Type Description <code>Chroma</code> <code>Chroma</code> <p>Loaded Chroma vector store.</p>"}]}